use ndarray::ArrayD;
use rand::SeedableRng;
use rand::seq::SliceRandom;
use rand_distr::Distribution;
use statrs::distribution::MultivariateNormal;
use std::cell::RefCell;
use std::fs::File;
use std::io::{BufWriter, Write};
use std::rc::Rc;
use plotters::prelude::*;
use plotters::style::RGBColor;

use gradflow::modules;
use gradflow::nn::{Linear, Module, ReLU, Sequential, Sigmoid};
use gradflow::optimizer::SGD;
use gradflow::tensor::{bce_loss, Tensor};

const RED: RGBColor = RGBColor(231, 0, 11);
const BLUE: RGBColor = RGBColor(21, 93, 252);

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Parameters
    let n_per_class = 200;
    let seed = 42u64; // reproducible
    let mut rng = rand::rngs::StdRng::seed_from_u64(seed);

    // Gaussian blob parameters: means and covariance matrices
    let mean0 = vec![-1.0f64, -0.5f64];
    let mean1 = vec![1.0f64, 0.5f64];

    // Covariance matrices as Vec<Vec<f64>>
    // Covariance matrices as flattened 1D vectors (row-major)
    let cov0 = vec![0.36, 0.1, 0.1, 0.36];
    let cov1 = vec![0.36, -0.1, -0.1, 0.36];

    let mvn0 = MultivariateNormal::new(mean0, cov0)?;
    let mvn1 = MultivariateNormal::new(mean1, cov1)?;

    // Storage
    let mut points: Vec<(f64, f64, u8)> = Vec::with_capacity(n_per_class * 2);

    // Generate class 0
    for _ in 0..n_per_class {
        let sample = mvn0.sample(&mut rng);
        points.push((sample[0], sample[1], 0));
    }

    // Generate class 1
    for _ in 0..n_per_class {
        let sample = mvn1.sample(&mut rng);
        points.push((sample[0], sample[1], 1));
    }

    // Optionally shuffle the dataset
    points.shuffle(&mut rng);

    // Write CSV
    let file = File::create("data/data.csv")?;
    let mut w = BufWriter::new(file);
    writeln!(w, "x,y,label")?;
    for (x, y, label) in &points {
        writeln!(w, "{:.6},{:.6},{}", x, y, label)?;
    }
    w.flush()?;
    println!("Wrote dataset to data.csv ({} samples).", points.len());

    

    // Training
    let model = Sequential::new(modules![
        Linear::new(2, 16),
        ReLU::new(),
        Linear::new(16, 8),
        ReLU::new(),
        Linear::new(8, 1),
        Sigmoid::new(),
    ]);
    let optimizer = SGD { params: model.parameters(), lr: 0.01 };

    for i in 0..100 {
        // Get data
        let x = Rc::new(RefCell::new(Tensor::new(ArrayD::from_elem(vec![2], 1.0), false)));
        let y_true = Rc::new(RefCell::new(Tensor::new(ArrayD::from_elem(vec![1], 2.0), false)));

        // Zero all the gradients
        optimizer.zero_grad();

        // Make predictions
        let y_pred = model.forward(x);

        // Compute loss and its gradients
        let loss = bce_loss(&y_pred, &y_true); // TO-DO: Fake loss
        loss.borrow_mut().backward();

        // Adjust learning weights
        optimizer.step();

        // Logging
        println!("Iteration {:?} | Loss: {:?}", i, loss.borrow().data);
    }

    Ok(())
}